a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta1", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
library(jagsUI)
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
?jags
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta1[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta1", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Bundle and summarize the data set passed to JAGS
str(bdata <- list(C = C, N = N, pop = as.numeric(pop), n.groups = n.groups,
wetness = wetness, n = n, X = X))
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * X[i]*wetness[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
plogis(4.3)
plogis(4.3 -26.6*1)
plogis(4.3 - 26.6*1)
exp(-.767)
exp(-.767+3.449)
h = function(x){(cos(50*x)+sin(20*x))^2}
x=h(runif(10))
estint = cumsum(x)/(1:10)
plot(estint)
x=h(runif(100))
estint = cumsum(x)/(1:100)
plot(estint)
set.seed(1)           # makes the experiment reproducible
m <- 2000             # number of simulated values
x <- 03                # observed data
x <- 0              # observed data
# Now simulate some random variables
theta <- rcauchy(m)                  # simulate m standard Cauchys
h <- pi * exp(-0.5*(x - theta)^2)    # who wants to write this over and over
Constant <- mean(h)                  # estimate normalizing constant
post.mean <- mean(theta * h)/mean(h)
estint = cumsum(h)/(1:m)
plot(estint)
library(devtools)
install_github(repo = "TsuPeiChiu/DNAshapeR", build_vignettes = TRUE)
install_github(repo = "TsuPeiChiu/DNAshapeR", build_vignettes = TRUE)
plot(c(1,1), c(2,3), col = "blue")
text(x,y, expression(italic(“Genus”))
text(1,2, expression(italic(“Genus”))
text(1,2, expression(italic("Genus"))
)
plot(c(1,1), c(2,3), col = "blue")
text(1,2.6, expression(italic("Genus")))
pdf("rplotds.pdf")
plot(c(1,1), c(2,3), col = "blue")
text(1,2.6, expression(italic("Genus")))
dev.off()
library(plotrix)
install.packages()
install.packages("plotrix")
library(plotrix)
plot(c(0,1), c(0,1), col = "white")
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col=NA,lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col=NA,lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.7, .1 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
plot(c(0,1), c(0,1), col = "white")
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.7, .1 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
points(c(.2,.5,.05, .7), c(.4,.7,.8,.1), pch = 19, col = "black")
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
?gl
group
#First let's make up some results hahaha
set.seed(10)
density <- runif(50, 1, 25)
sites <- 1:50
rpois(50, density*-.2)
?rnorm
counts <- rpois(50, density*-.2+rnorm(50, mean = 5, sd = 2))
counts <- rpois(50, density*.2+rnorm(50, mean = 5, sd = 2))
counts
shiny::runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
citation()
citeNatbib()
citeNatbib(R)
citeNatbib("R")
citation(nimble)
citation("nimble")
print(citation("nimble"), bibtex = TRUE)
library(rnoaa)
library(data.table)
library(sp)
library(rgeos)
meta <- read.csv("ghcnd-stations.csv")
setwd("~/Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project/Shinyapp")
meta <- read.csv("ghcnd-stations.csv")
res1 <- isd(usaf='011690', wban='99999', year=1993)
head(res1)
?isd_stations_search
Fakedata <- data.frame(Lat = c(33.9), lon = c(-83), Year = 2020)
isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)
isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[1,]
station <- isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[1,]
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020)
isd(usaf=station$usaf, wban=station$wban, year=2020)
station <- isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[2,]
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020)
?isd
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020, progress = T)
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020, additional = F, force = T)
Fakedata <- data.frame(Lat = c(33.9), lon = c(-83), Year = 2020)
station <- isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[2,]
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020, additional = F, force = T)
head(res1)
names(res1)
res1$temperature
Fakedata <- data.frame(Lat = c(30.9), lon = c(-83), Year = 2020)
station <- isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[2,]
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020, additional = F, force = T)
station <- isd_stations_search(lat =Fakedata$Lat, lon= Fakedata$lon, radius= 50)[1,]
res1 <- isd(usaf=station$usaf, wban=station$wban, year=2020, additional = F, force = T)
?getData
library(raster)
getData('worldclim', var = 'tmin', lon= -85.45714 , lat= 31.95954, res = 0.5)
install.packages("rWBclimate")
library(rWBclimate)
?rWBclimate
?`rWBclimate-package`
getData('worldclim', var = 'tmin', lon= -85.45714 , lat= 31.95954, res = 0.5)
install.packages("prism")
library("prism")
?get_prism_monthlys
get_prism_monthlys(type = "tmin", year = 2020, mon = 1)
h <- get_prism_monthlys(type = "tmin", year = 2020, mon = 1)
ls_prism_data(name=TRUE)
?prism_stack
RS <- prism_stack(ls_prism_data()[1,1]) ##raster file of data
plot(RS)
?crop
str(RS)
str(Fakedata)
Fakedata <- data.frame(Lat = c(30.9), lon = c(-83), Year = 2020)
Fakedata <- data.frame(Lat = c(30.9), lon = c(-83), Year = 2020)
coordinates(Fakedata) <- ~lon+Lat
points(Fakedata)
meep <- crop(RS, extent(RS, 30, 40, 15, 20))
plot(meep)
meep <- crop(RS, extent(RS, 30, 40, 2, 10))
plot(meep)
meep <- crop(RS, extent(RS, 30, 40, 2, 50))
plot(meep)
meep <- crop(RS, extent(RS, 10, 40, 2, 50))
plot(meep)
meep <- crop(RS, extent(RS, 1, 40, 2, 50))
plot(meep)
meep <- crop(RS, extent(RS, 1, 400, 2, 50))
plot(meep)
meep <- crop(RS, extent(RS, 4000, 5400, 2, 50))
meep <- crop(RS, extent(RS, 1000, 2400, 2, 50))
meep <- crop(RS, extent(RS, 1000, 2000, 2, 50))
meep <- crop(RS, extent(RS, 500, 800, 2, 50))
meep <- crop(RS, extent(RS, 500, 700, 2, 50))
meep <- crop(RS, extent(RS, 500, 600, 2, 50))
plot(meep)
meep <- crop(RS, extent(RS, 500, 600, 2, 200))
plot(meep)
meep <- crop(RS, extent(RS, 500, 600, 2, 500))
meep <- crop(RS, extent(RS, 300, 600, 2, 500))
plot(meep)
meep <- crop(RS, extent(RS, 300, 500, 2, 500))
plot(meep)
meep <- crop(RS, extent(RS, 300, 500, 500, 1500))
meep <- crop(RS, extent(RS, 300, 500, 500, 1000))
plot(meep)
meep <- crop(RS, extent(RS, 100, 500, 500, 1000))
plot(meep)
meep <- crop(RS, extent(RS, 100, 700, 500, 1000))
meep <- crop(RS, extent(RS, 100, 600, 500, 1000))
plot(meep)
meep <- crop(RS, extent(RS, 100, 600, 700, 1100))
plot(meep)
meep <- crop(RS, extent(RS, 100, 600, 700, 1200))
plot(meep)
plot_usmap("states",
include = c(.south_region), exclude= c("Texas", "Oklahoma"))
library(usmap)
plot_usmap("states",
include = c(.south_region), exclude= c("Texas", "Oklahoma"))
plot_usmap("states",
include = c(.south_region), exclude= c("Texas", "Oklahoma")) + meep
meep <- crop(RS, extent(RS, 100, 600, 700, 1250))
meep <- crop(RS, extent(RS, 100, 600, 700, 1350))
plot(meep)
meep <- crop(RS, extent(RS, 100, 600, 700, 1150))
plot(meep)
meep <- crop(RS, extent(RS, 100, 600, 700, 1180))
plot(meep)
meep <- crop(RS, extent(RS, 100, 500, 700, 1180))
plot(meep)
meep <- crop(RS, extent(RS, 100, 300, 700, 1180))
plot(meep)
meep <- crop(RS, extent(RS, 400, 600, 700, 1180))
plot(meep)
meep <- crop(RS, extent(RS, 300, 600, 700, 1180))
plot(meep)
meep <- crop(RS, extent(RS, 250, 600, 700, 1180))
plot(meep)
str(meep)
?numericInput
library(rshiny)
library(shiny)
?numericInput
plot_usmap("states",
include = c(.south_region), exclude= c("Texas", "Oklahoma"))+
gplot(meep)
library(ggplot2)
gplot
?gplot
??gplot
plot_usmap("states",
include = c(.south_region), exclude= c("Texas", "Oklahoma"))+
ggplot(meep)
plot(meep)
writeRaster(meep, filename = "2020_Jan_min")
get_prism_monthlys(type = "tmin", year = 2020, mon = 2)
get_prism_monthlys(type = "tmin", year = 2020, mon = 3)
points(Fakedata)
ls_prism_data()
RS2 <-prism_stack(ls_prism_data()[3,1]) #feb
RS3 <-prism_stack(ls_prism_data()[4,1]) #march
get_prism_monthlys(type = "tmax", year = 2019, mon = 8)
ls_prism_data()
RS4 <-prism_stack(ls_prism_data()[1,1]) #august 2019
meep2 <- crop(RS2, extent(RS2, 250, 600, 700, 1180))
plot(meep2)
writeRaster(meep2, filename = "2020_Feb_min")
meep3 <- crop(RS3, extent(RS3, 250, 600, 700, 1180))
writeRaster(meep3, filename = "2020_March_min")
meep4 <- crop(RS4, extent(RS4, 250, 600, 700, 1180))
writeRaster(meep4, filename = "2019_August_max")
Jan <- raster("2020_Jan_min.grd")
?extract
extract(Jan, Fakedata)
Feb <- raster("2020_Feb_min.grd")
March <- raster("2020_March_min.grd")
Aug <- raster("2019_August_max.grd")
?get_historical_precip
?prism
??prism
?get_prism_annual(type = "ppt", year = 2019)
get_prism_annual(type = "ppt", year = 2019)
ls_prism_data()
ls_prism_data()
getwd()
ls_prism_data()
get_prism_annual(type = "ppt", year = 2019)
ls_prism_data()
get_prism_monthlys(type = "ppt", year = 2019)
get_prism_monthlys(type = "ppt", year = 2019, m = 1:12)
ls_prism_data()
RSppt <-prism_stack(ls_prism_data()[1:12,1]) #year
plot(RSppt)
rsppt <- calc(RSppt, sum)
plot(rsppt)
meep5 <- crop(rsppt, extent(rsppt, 250, 600, 700, 1180))
plot(meep5)
writeRaster(meep5, filename = "2019_ppt")
Precip <<- raster("2019_ppt.grd")
Fakedata
Fakedata <- data.frame(Lat = c(30.9), lon = c(-83), Year = 2020)
predme <- Fakedata
coordinates(predme) <- ~lon+Lat
Jan <<- raster("2020_Jan_min.grd")
Feb <<- raster("2020_Feb_min.grd")
March <<- raster("2020_March_min.grd")
Aug <<- raster("2019_August_max.grd")
Precip <<- raster("2019_ppt.grd")
Fakedata$JANMINTEMP <- extract(Jan, predme)
Fakedata$FEBMINTEMP <- extract(Feb, predme)
Fakedata$MARMINTEMP <- extract(March, predme)
Fakedata$Prcp <- extract(Precip, predme)
Fakedata$AUGMAXTEMP <- extract(Aug, predme)
Fakedata
m_gam1 <<- readRDS("insect_predict.rds")
preds <- predict(m_gam1,Fakedata,type="response", se.fit = T)
str(Fakedata)
str(m_gam1)
Fakedata <- data.frame(Latitude = c(30.9), Longitude = c(-83), Year = 2020)
predme <- Fakedata
coordinates(predme) <- ~lon+Lat
Jan <<- raster("2020_Jan_min.grd")
Feb <<- raster("2020_Feb_min.grd")
March <<- raster("2020_March_min.grd")
coordinates(predme) <- ~Longitude+Latitude
Jan <<- raster("2020_Jan_min.grd")
Feb <<- raster("2020_Feb_min.grd")
March <<- raster("2020_March_min.grd")
Aug <<- raster("2019_August_max.grd")
Precip <<- raster("2019_ppt.grd")
Fakedata$JANMINTEMP <- extract(Jan, predme)
Fakedata$FEBMINTEMP <- extract(Feb, predme)
Fakedata$MARMINTEMP <- extract(March, predme)
Fakedata$Prcp <- extract(Precip, predme)
Fakedata$AUGMAXTEMP <- extract(Aug, predme)
preds <- predict(m_gam1,Fakedata,type="response", se.fit = T)
preds$fit
runApp()
View(Fakedata)
runApp()
runApp()
Fakedata <- data.frame(Longitude = -35 ,Latitude = -15)
usmap_transform(Fakedata)
?numericInput
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
