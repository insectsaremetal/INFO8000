hist(Xt)
plot(cummean(Xt), type="l", xlab="Sample", ylab="Cumulative mean of Xt")
abline(h = lambda , col = "red")
}
MCMC.pois(15, 2, 10, 10000)
## Normal Distribution w/normal generating distribution
MCMC.norm2 <- function(init, A, mu, var,t){
cummean <- function(x)
cumsum(x) / seq_along(x)
Xt <- u1 <- xcand <- fxcand <- fxt <- r <- u2 <- rep(NA,t) #setup
Xt[1] <- init #starting value
for(i in 1:t){
u1[i] <- rnorm(1,mean = 0, sd = sqrt(abs(A))) #first random, now normal; can't be negative
xcand[i] <- Xt[i] + u1[i] #candidate
fxcand[i] <- 1/(sqrt(var*3.14))*exp(-(xcand[i]-mu)^2/(2*var))
fxt[i] <- 1/(sqrt(var*3.14))*exp(-(Xt[i]-mu)^2/(2*var))
r[i]<- fxcand[i]/fxt[i] #candidate ratio
u2[i] <- runif(1,0,1) #second random
Xt[i+1] <- ifelse(r[i] > u2[i], xcand[i], Xt[i]) #choose new starting value
}
par(mfrow = c(2,2))
plot(0:t, Xt, type = "l", main = "Xt Over Time \nNormal", xlab = "Time Step", ylab = "Xt")
hist(Xt)
plot(cummean(Xt), type="l", xlab="Sample", ylab="Cumulative mean of Xt")
abline(h = mu, col = "red")
}
MCMC.norm2(.2, A =.5, mu = 0, var =1, t = 10000)
### Normal Distribution w/ uniform generating distribution and adaptive A
MCMC.norm3 <- function(init, A, mu, var,t, min, max, change){
cummean <- function(x)
cumsum(x) / seq_along(x)
Xt <- u1 <- xcand <- fxcand <- fxt <- r <- u2 <- a.rate <- A.list <- rep(NA,t) #setup
Xt[1] <- init #starting value
A.list[1] <- A
for(i in 1:t){
u1[i] <- runif(1,0,1) #first random
xcand[i] <- Xt[i] + 2*A*(u1[i]-0.5) #candidate
fxcand[i] <- 1/(sqrt(var*3.14))*exp(-(xcand[i]-mu)^2/(2*var))
fxt[i] <- 1/(sqrt(var*3.14))*exp(-(Xt[i]-mu)^2/(2*var))
r[i]<- fxcand[i]/fxt[i] #candidate ratio
u2[i] <- runif(1,0,1) #second random
Xt[i+1] <- ifelse(r[i] > u2[i], xcand[i], Xt[i]) #choose new starting value
a.rate[i] <- r[i] > u2[i]
if(i %in% seq(10,t, by = 10)){ #every tenth
A <- ifelse(sum(a.rate[(i-9):i])/10 > max, A+change,
ifelse(sum(a.rate[(i-9):i])/10 < min, A-change, A))
A.list[i] <- A
}
}
#return(a.rate)
par(mfrow = c(2,2))
plot(0:t, Xt, type = "l", main = "Xt Over Time \nNormal", xlab = "Time Step", ylab = "Xt")
hist(Xt)
plot(cummean(Xt), type="l", xlab="Time Step", main ="Cumulative mean \nof Xt", ylab = "Cummean")
abline(h = mu, col = "red")
plot(seq(10, t, by = 10), A.list[seq(10, t, by = 10)], type = "l",
main = "Adapative Tuning \nof A Over Time", xlab = "Time Step", ylab = "Value of A")
}
MCMC.norm3(.2, A =.5, mu = 0, var =1, t = 10000,
min = .2, max =.5, change = .2)
### And finally, normal w/ normal generating and adaptive A!
MCMC.norm4 <- function(init, A, mu, var,t, min, max, change){
cummean <- function(x)
cumsum(x) / seq_along(x)
Xt <- u1 <- xcand <- fxcand <- fxt <- r <- u2 <- a.rate <- A.list <- rep(NA,t) #setup
Xt[1] <- init #starting value
A.list[1] <- A
for(i in 1:t){
u1[i] <- rnorm(1,0,sd = sqrt(abs(A))) #first random
xcand[i] <- Xt[i] + u1[i] #candidate
fxcand[i] <- 1/(sqrt(var*3.14))*exp(-(xcand[i]-mu)^2/(2*var))
fxt[i] <- 1/(sqrt(var*3.14))*exp(-(Xt[i]-mu)^2/(2*var))
r[i]<- fxcand[i]/fxt[i] #candidate ratio
u2[i] <- runif(1,0,1) #second random
Xt[i+1] <- ifelse(r[i] > u2[i], xcand[i], Xt[i]) #choose new starting value
a.rate[i] <- r[i] > u2[i]
if(i %in% seq(10,t, by = 10)){ #every tenth
A <- ifelse(sum(a.rate[(i-9):i])/10 > max, A+change,
ifelse(sum(a.rate[(i-9):i])/10 < min, A-change, A))
A.list[i] <- A
}
}
#return(a.rate)
par(mfrow = c(2,2))
plot(0:t, Xt, type = "l", main = "Xt Over Time \nNormal", xlab = "Time Step", ylab = "Xt")
hist(Xt)
plot(cummean(Xt), type="l", xlab="Time Step", main ="Cumulative mean \nof Xt", ylab = "Cummean")
abline(h = mu, col = "red")
plot(seq(10, t, by = 10), A.list[seq(10, t, by = 10)], type = "l",
main = "Adapative Tuning \nof A Over Time", xlab = "Time Step", ylab = "Value of A")
}
MCMC.norm4(.2, A =.5, mu = 0, var =1, t = 10000,
min = .2, max =.5, change = .2)
n.groups <- 3
n.sample <- 10
n <- n.groups * n.sample
x <- rep(1:n.groups, rep(n.sample, n.groups))
pop <- factor(x, labels = c("Jura", "Black Forest", "Alps"))
wetness.Jura <- sort(runif(n.sample, 0, 1))
wetness.BlackF <- sort(runif(n.sample, 0, 1))
wetness.Alps <- sort(runif(n.sample, 0, 1))
wetness <- c(wetness.Jura, wetness.BlackF, wetness.Alps)
X <- runif(length(wetness))
Xwet <- X * wetness
N <- round(runif(n, 10, 50) )		# Get discrete Uniform values
Xmat <- model.matrix(~ pop*wetness)
print(Xmat, dig = 2)
beta.vec <- c(-4, 1, 2, 6, 2, -5)
lin.pred <- Xmat[,] %*% beta.vec	# Value of lin.predictor
exp.p <- exp(lin.pred) / (1 + exp(lin.pred)) # Expected proportion
C <- rbinom(n = n, size = N, prob = exp.p) # Add binomial noise
hist(C)					# Inspect what we've created
par(mfrow = c(2,1))
matplot(cbind(wetness[1:10], wetness[11:20], wetness[21:30]), cbind(exp.p[1:10],
exp.p[11:20], exp.p[21:30]), ylab = "Expected black", xlab = "", col = c("red","green","blue"),
pch = c("J","B","A"), lty = "solid", type = "b", las = 1, cex = 1.2, main = "", lwd = 1)
matplot(cbind(wetness[1:10], wetness[11:20], wetness[21:30]), cbind(C[1:10]/N[1:10],
C[11:20]/N[11:20], C[21:30]/N[21:30]), ylab = "Observed black", xlab = "Wetness index",
col = c("red","green","blue"), pch = c("J","B","A"), las = 1, cex = 1.2, main = "")
par(mfrow = c(1,1))
# Bundle and summarize the data set passed to JAGS
str(bdata <- list(C = C, N = N, pop = as.numeric(pop), n.groups = n.groups,
wetness = wetness, n = n))
# Specify model in BUGS language
cat(file = "glm.txt","
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta1[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta1", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
library(jagsUI)
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
?jags
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta1[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta1", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Bundle and summarize the data set passed to JAGS
str(bdata <- list(C = C, N = N, pop = as.numeric(pop), n.groups = n.groups,
wetness = wetness, n = n, X = X))
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * Xwet[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
# Specify model in BUGS language
cat(file = "glm.txt",
"
model {
# Priors
for (i in 1:n.groups){
alpha[i] ~ dnorm(0, 0.01)		# Intercepts
beta[i] ~ dnorm(0, 0.01)		# Slopes
}
beta2 ~ dnorm(0, 0.01)
gamma ~ dnorm(0, 0.01)
# Likelihood
for (i in 1:n) {
C[i] ~ dbin(p[i], N[i])
logit(p[i]) <- alpha[pop[i]] + beta[pop[i]]*wetness[i] + beta2 * X[i] + gamma * X[i]*wetness[i]
# Fit assessments: Pearson residuals and posterior predictive check
Presi[i] <- (C[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))	# Pearson resi
C.new[i] ~ dbin(p[i], N[i])		# Create replicate data set
Presi.new[i] <- (C.new[i]-N[i]*p[i]) / sqrt(N[i]*p[i]*(1-p[i]))
}
# Derived quantities
# Recover the effects relative to baseline level (no. 1)
a.effe2 <- alpha[2] - alpha[1]		# Intercept Black Forest vs. Jura
a.effe3 <- alpha[3] - alpha[1]		# Intercept Alps vs. Jura
b.effe2 <- beta[2] - beta[1]		# Slope Black Forest vs. Jura
b.effe3 <- beta[3] - beta[1]		# Slope Alps vs. Jura
}
")
# Inits function
# inits <- function(){ list(alpha = rlnorm(n.groups, 3, 1), beta = rlnorm(n.groups, 2, 1))} # Note log-normal inits (old, from original)
inits <- function(){ list(alpha = runif(n.groups), beta = runif(n.groups))}  ######  Note other try for inits for JAGS, which works #######
# Parameters to estimate
params <- c("alpha", "beta", "beta2", "gamma", "a.effe3", "a.effe2", "b.effe2", "b.effe3")
# MCMC settings
na <- 1000  ;  nc <- 3  ;  ni <- 5000  ;  nb <- 1000  ;  nt <- 4
# Call JAGS, check convergence and summarize posteriors
out <- jags(bdata, inits, params, "glm.txt", n.adapt = na, n.thin = nt, n.chains = nc,
n.burnin = nb, n.iter = ni, parallel = TRUE)
plogis(4.3)
plogis(4.3 -26.6*1)
plogis(4.3 - 26.6*1)
exp(-.767)
exp(-.767+3.449)
h = function(x){(cos(50*x)+sin(20*x))^2}
x=h(runif(10))
estint = cumsum(x)/(1:10)
plot(estint)
x=h(runif(100))
estint = cumsum(x)/(1:100)
plot(estint)
set.seed(1)           # makes the experiment reproducible
m <- 2000             # number of simulated values
x <- 03                # observed data
x <- 0              # observed data
# Now simulate some random variables
theta <- rcauchy(m)                  # simulate m standard Cauchys
h <- pi * exp(-0.5*(x - theta)^2)    # who wants to write this over and over
Constant <- mean(h)                  # estimate normalizing constant
post.mean <- mean(theta * h)/mean(h)
estint = cumsum(h)/(1:m)
plot(estint)
library(devtools)
install_github(repo = "TsuPeiChiu/DNAshapeR", build_vignettes = TRUE)
install_github(repo = "TsuPeiChiu/DNAshapeR", build_vignettes = TRUE)
plot(c(1,1), c(2,3), col = "blue")
text(x,y, expression(italic(“Genus”))
text(1,2, expression(italic(“Genus”))
text(1,2, expression(italic("Genus"))
)
plot(c(1,1), c(2,3), col = "blue")
text(1,2.6, expression(italic("Genus")))
pdf("rplotds.pdf")
plot(c(1,1), c(2,3), col = "blue")
text(1,2.6, expression(italic("Genus")))
dev.off()
library(plotrix)
install.packages()
install.packages("plotrix")
library(plotrix)
plot(c(0,1), c(0,1), col = "white")
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col=NA,lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col=NA,lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.7, .1 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
plot(c(0,1), c(0,1), col = "white")
draw.circle(.2, .4 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.5, .7 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.05 ,.8,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
draw.circle(.7, .1 ,.3, nv=100,border=NULL,col="blue",lty=1,density=NULL,
angle=45,lwd=1)
points(c(.2,.5,.05, .7), c(.4,.7,.8,.1), pch = 19, col = "black")
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
?gl
group
#First let's make up some results hahaha
set.seed(10)
density <- runif(50, 1, 25)
sites <- 1:50
rpois(50, density*-.2)
?rnorm
counts <- rpois(50, density*-.2+rnorm(50, mean = 5, sd = 2))
counts <- rpois(50, density*.2+rnorm(50, mean = 5, sd = 2))
counts
shiny::runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
?pch
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
runApp('Desktop/U Georgia/INFO8000/Semester_Project/INFO8000/Semester_Project')
meep <- read.csv("Outbreak_data.csv")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set directory to file location
meep <- read.csv("Outbreak_data.csv")
head(meep)
newdat <- data.frame(Year = c(2014, 2014, 2014), Acres = c(.1, .2, .3), Latitude = c(31.96, 31.95, 31.93), Longitude = c(-85.45365, -85.45345, -85.45264))
for (i in 1: nrow(newdat)){
daymet <- download_daymet(site = "mysite",
lat = newat$Latitude[i],
lon = newdat$Longitude[i],
start = newdat$year[i] - 1,
end = newdat$year[i],
internal = TRUE,
simplify = F) # returns tidy data!
newdat$JANMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[366:396]) #my year
newdat$FEBMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[397:424]) #my year
newdat$MARMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[425:455]) #my year
newdat$Prcp[i] <- sum(daymet$data$prcp..mm.day.[365:730]) #my year
newdat$AUGMAXTEMP[i] <- mean(daymet$data$tmax..deg.c.[213:243]) #year BEFORE
}
for (i in 1: nrow(newdat)){
daymet <- download_daymet(site = "mysite",
lat = newat$Latitude[i],
lon = newdat$Longitude[i],
start = newdat$Year[i] - 1,
end = newdat$Year[i],
internal = TRUE,
simplify = F) # returns tidy data!
newdat$JANMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[366:396]) #my year
newdat$FEBMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[397:424]) #my year
newdat$MARMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[425:455]) #my year
newdat$Prcp[i] <- sum(daymet$data$prcp..mm.day.[365:730]) #my year
newdat$AUGMAXTEMP[i] <- mean(daymet$data$tmax..deg.c.[213:243]) #year BEFORE
}
for (i in 1: nrow(newdat)){
daymet <- download_daymet(site = "mysite",
lat = newdat$Latitude[i],
lon = newdat$Longitude[i],
start = newdat$Year[i] - 1,
end = newdat$Year[i],
internal = TRUE,
simplify = F) # returns tidy data!
newdat$JANMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[366:396]) #my year
newdat$FEBMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[397:424]) #my year
newdat$MARMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[425:455]) #my year
newdat$Prcp[i] <- sum(daymet$data$prcp..mm.day.[365:730]) #my year
newdat$AUGMAXTEMP[i] <- mean(daymet$data$tmax..deg.c.[213:243]) #year BEFORE
}
coordinates(newdat) <- ~Longitude+Latitude
proj4string(newdat) <-CRS ("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
newdat
for (i in 1: nrow(newdat)){
daymet <- download_daymet(site = "mysite",
lat = newdat$Latitude[i],
lon = newdat$Longitude[i],
start = newdat$Year[i] - 1,
end = newdat$Year[i],
internal = TRUE,
simplify = F) # returns tidy data!
newdat$JANMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[366:396]) #my year
newdat$FEBMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[397:424]) #my year
newdat$MARMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[425:455]) #my year
newdat$Prcp[i] <- sum(daymet$data$prcp..mm.day.[365:730]) #my year
newdat$AUGMAXTEMP[i] <- mean(daymet$data$tmax..deg.c.[213:243]) #year BEFORE
}
newdat
newdat <- data.frame(Year = c(2014, 2014, 2014), Acres = c(.1, .2, .3), Latitude = c(31.96, 31.95, 31.93), Longitude = c(-85.45365, -85.45345, -85.45264))
for (i in 1: nrow(newdat)){
daymet <- download_daymet(site = "mysite",
lat = newdat$Latitude[i],
lon = newdat$Longitude[i],
start = newdat$Year[i] - 1,
end = newdat$Year[i],
internal = TRUE,
simplify = F) # returns tidy data!
newdat$JANMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[366:396]) #my year
newdat$FEBMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[397:424]) #my year
newdat$MARMINTEMP[i] <- mean(daymet$data$tmin..deg.c.[425:455]) #my year
newdat$Prcp[i] <- sum(daymet$data$prcp..mm.day.[365:730]) #my year
newdat$AUGMAXTEMP[i] <- mean(daymet$data$tmax..deg.c.[213:243]) #year BEFORE
}
newdat
head(meep)
rbind(newdat, meep)
?rbind
?merge
meee <- merge(newdat, meep, all.x = T)
meee <- merge(newdat, meep, all.x = T, all.y = T)
head(mee)
head(meee)
meee <- merge(newdat, meep, all = T)
head(meee)
View(newdat)
newdat <- data.frame(Year = c(2014, 2014, 2014), Acres = c(.1, .2, .3), Latitude = c(31.96, 31.95, 31.93), Longitude = c(-85.45365, -85.45345, -85.45264))
write.csv(newdat,"Practice_data.csv")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
